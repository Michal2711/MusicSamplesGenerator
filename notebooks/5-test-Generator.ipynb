{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data.audio_dataset import AudioSpectrogramDataset\n",
    "from models.Generator3 import Generator3\n",
    "from models.Discriminator3 import Discriminator3\n",
    "from models.PGAN_model.PGenerator2 import PGenerator2\n",
    "from models.PGAN_model.PGenerator import PGenerator\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# z_dim = 100\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "latent_dim = 32\n",
    "# learning_rate = 0.0002\n",
    "# beta1 = 0.5\n",
    "\n",
    "# base_directory = \"../data/raw/NSynth/audio\"\n",
    "base_directory = \"../data/raw/Bass\"\n",
    "\n",
    "dataset = AudioSpectrogramDataset(base_directory=base_directory, spectro_type='mel')\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 160])\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(dataloader))\n",
    "print(first_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10240])\n",
      "torch.Size([1, 256, 8, 5])\n",
      "block number: 0 with shape input: torch.Size([1, 256, 8, 5])\n",
      "block number: 1 with shape input: torch.Size([1, 256, 16, 10])\n",
      "block number: 2 with shape input: torch.Size([1, 128, 32, 20])\n",
      "first block output before RGB shape: torch.Size([1, 64, 64, 40])\n",
      "first block output after RGB shape: torch.Size([1, 1, 64, 40])\n",
      "torch.Size([1, 1, 64, 40])\n"
     ]
    }
   ],
   "source": [
    "p_latent_dim = 100\n",
    "dummy_latent_vector = torch.randn((1, p_latent_dim))\n",
    "generator = PGenerator2()\n",
    "generator.add_next_block(new_depth=256)\n",
    "generator.add_next_block(new_depth=128)\n",
    "generator.add_next_block(new_depth=64)\n",
    "dummy_output = generator(dummy_latent_vector)\n",
    "print(dummy_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGenerator2(\n",
      "  (blocks): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "        (1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "        (3): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normalizationLayer): NormalizationLayer()\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=10240, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (base_block): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 16, 10])\n",
      "torch.Size([1, 128, 33, 21])\n",
      "torch.Size([1, 64, 67, 43])\n",
      "torch.Size([1, 32, 134, 86])\n",
      "torch.Size([1, 16, 268, 172])\n",
      "torch.Size([1, 1, 256, 173])\n",
      "torch.Size([1, 1, 256, 173])\n"
     ]
    }
   ],
   "source": [
    "dummy_latent_vector = torch.randn((1, latent_dim))\n",
    "generator = Generator3(height=256, width=172, transformation_type='mel', latent_dim=latent_dim)\n",
    "dummy_output = generator(dummy_latent_vector)\n",
    "print(dummy_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy shape: torch.Size([1, 1, 256, 173])\n",
      "torch.Size([1, 16, 268, 172])\n",
      "torch.Size([1, 32, 134, 86])\n",
      "torch.Size([1, 64, 67, 43])\n",
      "torch.Size([1, 128, 33, 21])\n",
      "torch.Size([1, 256, 16, 10])\n",
      "torch.Size([1, 40960])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator3(height=256, width=173, transformation_type='mel')\n",
    "print(f'dummy shape: {dummy_output.shape}')\n",
    "discriminator_output = discriminator(dummy_output)\n",
    "print(discriminator_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PGenerator(nn.Module):\n",
    "    def __init__(self, h_init=8, w_init=5, latent_dim=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h_init = h_init\n",
    "        self.w_init = w_init\n",
    "        self.latent_dim = latent_dim\n",
    "        self.stride = 1\n",
    "        self.kernel_size = 3\n",
    "        self.padding = 1\n",
    "\n",
    "        self.LeakyRelu = nn.LeakyReLU(0.2)\n",
    "        self.scaleLayers = nn.ModuleList()\n",
    "        self.toRGBLayers = nn.ModuleList()\n",
    "    \n",
    "        self.initFirstLinearLayer()\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Linear(self.latent_dim, 256 * self.h_init * self.w_init))\n",
    "\n",
    "        # Create a list to hold all our deconv layers\n",
    "        self.deconv_blocks = nn.ModuleList()\n",
    "\n",
    "        self.deconv_blocks.append(self._block(256, 256, kernel_size=3, stride = 1, padding = 1))\n",
    "\n",
    "        # Final layer to produce the image\n",
    "        self.deconv_blocks.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(256, 1, kernel_size=(1,1)),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def initFirstLinearLayer(self):\n",
    "        pass\n",
    "\n",
    "    def _block(self, in_channels, out_channels, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 256, self.h_init, self.w_init)\n",
    "        \n",
    "        print(out.size())\n",
    "\n",
    "        for block in self.deconv_blocks:\n",
    "            out = block(out)\n",
    "            print(out.size())\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 8, 5])\n",
      "torch.Size([1, 256, 8, 5])\n",
      "torch.Size([1, 1, 8, 5])\n",
      "torch.Size([1, 1, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "dummy_latent_vector = torch.randn((1, 100))\n",
    "generator = PGenerator(8, 5, latent_dim=100)\n",
    "dummy_output = generator(dummy_latent_vector)\n",
    "print(dummy_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class NormalizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NormalizationLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, epsilon=1e-8):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "\n",
    "class PGenerator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 init_depth=256, \n",
    "                 init_size=(8, 5),\n",
    "                 latent_dim=100,\n",
    "                 output_depth=1,\n",
    "                 init_scale=40,\n",
    "                 LReLU_negative_slope=0.2,\n",
    "                 toRGBActivation=None,\n",
    "                 normalization=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_depth = init_depth # liczba kanalow przy najmniejszej rozdzielczosci\n",
    "        self.output_depth = output_depth # to ile kanalow wyjsciowych\n",
    "        self.init_scale = init_scale # 8x5 rozmiar spectrogramu na poczatku\n",
    "        self.init_size = init_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.LReLU_negative_slope = LReLU_negative_slope # parametr LReLU\n",
    "        self.kernel_size = 3\n",
    "        self.padding = 1\n",
    "        self.normalization = normalization\n",
    "        self.toRGBActivation=toRGBActivation # funkcja aktywacji dla warsty toRGB, jesli None to uzyjemy identity\n",
    "\n",
    "        self.depths = [init_depth]\n",
    "\n",
    "        self.LeakyRelu = nn.LeakyReLU(negative_slope=self.LReLU_negative_slope)\n",
    "        self.scaleLayers = nn.ModuleList()\n",
    "        self.toRGBLayers = nn.ModuleList()\n",
    "\n",
    "        self.normalizationLayer = None\n",
    "        if normalization:\n",
    "            self.normalizationLayer = NormalizationLayer()\n",
    "\n",
    "        self.initFirstLinearLayer()\n",
    "        self.initScale0Layer()\n",
    "\n",
    "        self.alpha = 0\n",
    "\n",
    "    def initFirstLinearLayer(self):\n",
    "        self.l1 = nn.Linear(self.latent_dim, self.init_depth * self.init_size[0] * self.init_size[1])\n",
    "\n",
    "    def initScale0Layer(self):\n",
    "        self.groupScale0 = nn.ModuleList()\n",
    "\n",
    "        self.groupScale0.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.depths[0],\n",
    "                out_channels=self.depths[0],\n",
    "                kernel_size=self.kernel_size,\n",
    "                padding=self.padding\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.toRGBLayers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.depths[0],\n",
    "                out_channels=self.output_depth,\n",
    "                kernel_size=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def getOutputSize(self):\n",
    "        if type(self.init_size) == tuple:\n",
    "            size_h = int(self.init_size[0] * (2**(len(self.toRGBLayers))))\n",
    "            size_w = int(self.init_size[1] * (2**(len(self.toRGBLayers))))\n",
    "            return (size_h, size_w)\n",
    "        else:\n",
    "            size = self.init_size * (2**(len(self.toRGBLayers)))\n",
    "            return (size, size)\n",
    "\n",
    "    def _block(self, in_channels, out_channels, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], self.init_depth, self.init_size[0], self.init_size[1])\n",
    "        \n",
    "        print(out.size())\n",
    "\n",
    "        for block in self.deconv_blocks:\n",
    "            out = block(out)\n",
    "            print(out.size())\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n"
     ]
    }
   ],
   "source": [
    "PGenerator = PGenerator()\n",
    "output_size = PGenerator.getOutputSize()\n",
    "print(output_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
